{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: what model can answer this question?\n",
    "\n",
    "You now have a fairly substantial starting toolbox of supervised learning methods that you can use to tackle a host of exciting problems. \n",
    "\n",
    "For each problem below, identify which supervised learning method(s) would be best for addressing that particular problem. Explain your reasoning and discuss your answers with your mentor.\n",
    "\n",
    "- __Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.__\n",
    "\n",
    "    If there are a very high number of features, I would start with LASSO regression to see if there are any unhelpful features that can be dropped. PCA would also help me reduce the number of features.\n",
    "    \n",
    "    I would then follow up with a regression model (e.g. vanilla regression, ridge regression).\n",
    "    \n",
    "\n",
    "- __You have more features (columns) than rows in your dataset.__\n",
    "\n",
    "    I would first use PCA to reduce the number of features while still explaining much of the variance in the dataset. I would try to get to a point where I have more rows than features in my dataset. Then, assuming it is a regression problem, I would use LASSO regression to drop some more features. \n",
    "    \n",
    "    PCA must come first. Regression models (i.e. vanilla / ridge / LASSO) will not work if there are more features than rows, because there are no degrees of freedom of # columns > # rows (similar to having 2 equations to solve for 3 variables – impossible!).\n",
    "    \n",
    "\n",
    "- __Identify the most important characteristic predicting likelihood of being jailed before age 20.__\n",
    "\n",
    "    Any logistic regression model could be used in this situation (e.g. decision tree classifier, random forest classifier, etc.). A decision tree or a random forest classifier would be particularly helpful, as there are likely a great deal of continuous and categorical features that may not be normally distributed. \n",
    "    \n",
    "    A KNN model would not be useful, because KNN models to not provide the likelihood of a datapoint belonging to a particular category – just the category that the datapoint is more likely to belong to, based on n number of neighbors (which can be changed, and which may change the category). \n",
    "    \n",
    "\n",
    "- __Implement a filter to “highlight” emails that might be important to the recipient.__\n",
    "\n",
    "    I would use a Naive Bayes classifier, as my target outcome is just a category.\n",
    "    \n",
    "\n",
    "- __You have 1000+ features.__\n",
    "    \n",
    "    If it is a regression problem, BUT there are more features than rows, I would first use PCA to reduce the number of features until it is less than the number of rows. Then, I would be able to use LASSO or any other type of regression, since a base assumption of regression models is that there are more rows than columns.\n",
    "\n",
    "    Assuming it is a regression problem AND there are more rows than features, the order would not matter as much. I could use LASSO regression first to drop some features, then use PCA to reduce the number of features while still explaining much of the variance in the dataset.\n",
    "    \n",
    "\n",
    "- __Predict whether someone who adds items to their cart on a website will purchase the items.__\n",
    "\n",
    "    I would use any classification model (e.g. decision tree classifier, random forest classifier, logistic regression, Naive Bayes, KNN, gradient boost, SVM, etc.). Choosing a particular model would depend on the actual data in the dataset.\n",
    "    \n",
    "\n",
    "- __Your dataset dimensions are 982400 x 500.__\n",
    "\n",
    "    Assuming it is a regression problem, I would first use LASSO regression to drop some features. Then I would use PCA to reduce the number of features (500) while still explaining much of the variance in the dataset's 982400 rows.\n",
    "    \n",
    "\n",
    "- __Identify faces in an image.__\n",
    "\n",
    "    If `has_face` or `num_faces` is my target outcome (i.e. `True` or `False`, or 0 ~ infinity), and assuming the data is already collected and ready to clean and analyze, I could use any classification model to determine whether an image has a face.\n",
    "    \n",
    "    NOTE: neural networks have not been covered in curriculum yet, would be used for a problem like this.\n",
    "    \n",
    "\n",
    "- __Predict which of 3 flavors of ice cream will be most popular with boys vs girls.__\n",
    "\n",
    "    Hypothesis testing would be the best way to predict the most popular flavors for boys vs. girls – by comparing all flavor pairings using ANOVA or T-tests to get the top 3 flavors for boys, and then doing the same for girls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA (2+ groups, continuous)\n",
    "# T-Test (2 groups, continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
