{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: what model can answer this question?\n",
    "\n",
    "You now have a fairly substantial starting toolbox of supervised learning methods that you can use to tackle a host of exciting problems. \n",
    "\n",
    "For each problem below, identify which supervised learning method(s) would be best for addressing that particular problem. Explain your reasoning and discuss your answers with your mentor.\n",
    "\n",
    "- __Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.__\n",
    "\n",
    "    I would start with LASSO regression (as my target outcome is continuous) to see if there are any unhelpful features that can be dropped, and then follow up with a ridge regression (like vanilla regression, but optimizes for variance found in test data).\n",
    "    \n",
    "\n",
    "- __You have more features (columns) than rows in your dataset.__\n",
    "\n",
    "    Assuming it is a regression problem, I would first use LASSO regression to drop some features. Then I would use PCA to reduce the number of features while still explaining much of the variance in the dataset.\n",
    "    \n",
    "\n",
    "- __Identify the most important characteristic predicting likelihood of being jailed before age 20.__\n",
    "\n",
    "    I would use a decision tree or a random forest classifier (for higher accuracy), as there are likely a great deal of features (both continuous and categorical) that may not be normally distributed. \n",
    "    \n",
    "    I could also use a KNN model to see which category that a new observation point's \"nearest neighbors\" belong to.\n",
    "    \n",
    "\n",
    "- __Implement a filter to “highlight” emails that might be important to the recipient.__\n",
    "\n",
    "    I would use a Naive Bayes classifier, as my target outcome is a category.\n",
    "    \n",
    "\n",
    "- __You have 1000+ features.__\n",
    "\n",
    "    Assuming it is a regression problem, I would first use LASSO regression to drop some features. Then I would use PCA to reduce the number of features while still explaining much of the variance in the dataset.\n",
    "    \n",
    "\n",
    "- __Predict whether someone who adds items to their cart on a website will purchase the items.__\n",
    "\n",
    "    I would use a decision tree or a random forest classifier (for higher accuracy), as there are likely a great deal of features (both continuous and categorical) that may not be normally distributed.\n",
    "    \n",
    "\n",
    "- __Your dataset dimensions are 982400 x 500.__\n",
    "\n",
    "    Assuming it is a regression problem, I would first use LASSO regression to drop some features. Then I would use PCA to reduce the number of features (500) while still explaining much of the variance in the dataset's 982400 rows.\n",
    "    \n",
    "\n",
    "- __Identify faces in an image.__\n",
    "\n",
    "    ???\n",
    "    \n",
    "\n",
    "- __Predict which of 3 flavors of ice cream will be most popular with boys vs girls.__\n",
    "\n",
    "    ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
