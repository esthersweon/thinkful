{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRILL: Random Forest, Third Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('https://www.dropbox.com/s/0so14yudedjmm5m/LoanStats3d.csv?dl=1', skipinitialspace=True, header=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df = df[300000:]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = df.select_dtypes(include=['object'])\n",
    "for i in categorical:\n",
    "    column = categorical[i]\n",
    "    print(i, column.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ID and Interest Rate to numeric.\n",
    "df['id'] = pd.to_numeric(df['id'], errors='coerce')\n",
    "df['int_rate'] = pd.to_numeric(df['int_rate'].str.strip('%'), errors='coerce')\n",
    "\n",
    "# Drop other columns with many unique variables\n",
    "df.drop(['url', 'emp_title', 'zip_code', 'earliest_cr_line', 'revol_util',\n",
    "            'sub_grade', 'addr_state', 'desc'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:-2]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "X = df.drop('loan_status', 1)\n",
    "Y = df['loan_status']\n",
    "X = pd.get_dummies(X)\n",
    "X = X.dropna(axis=1)\n",
    "\n",
    "cross_val_score(rfc, X, Y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Attempt\n",
    "\n",
    "Get rid of as much data as possible without dropping below an average of 90% accuracy in a 10-fold cross validation.\n",
    "\n",
    "First, dive into the data that we have and see which features are most important. This can be the raw features or the generated dummies. You may want to use PCA or correlation matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df = df.drop('id', 1)\n",
    "df = df.drop('member_id', 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make term number\n",
    "df.term = df.term.map({' 60 months': 60, ' 36 months': 36})\n",
    "\n",
    "# Make emp_length number\n",
    "df['emp_length'] = df['emp_length'].str.extract('(\\d+)')\n",
    "df = df.dropna(subset=['emp_length'])\n",
    "df['emp_length'] = df['emp_length'].astype(int)\n",
    "\n",
    "# Make grade, issue_d, last_pymnt_d, next_pymnt_d, last_credit_pull_d,\n",
    "# home_ownership, purpose, loan_status, initial_list_status into dummies later\n",
    "\n",
    "# Condense verification_status and make into boolean\n",
    "df.verification_status = df.verification_status.map({'Source Verified': 1, 'Not Verified': 0, 'Verified': 1})\n",
    "\n",
    "# Drop pymnt_plan and application_type and verification_status_joint (only 1 value each) \n",
    "# and title (purely qualitative and inconsistent)\n",
    "df = df.drop('pymnt_plan', 1)\n",
    "df = df.drop('application_type', 1)\n",
    "df = df.drop('verification_status_joint', 1)\n",
    "df = df.drop('title', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_groups = df.columns.to_series().groupby(df.dtypes).groups\n",
    "dtype_dict = {k.name: v for k, v in dtype_groups.items()}\n",
    "all_num_cols = list(dtype_dict['float64']) + list(dtype_dict['int64']) + ['loan_status']\n",
    "\n",
    "df = df[all_num_cols]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "sns.heatmap(df.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_num_components = [15, 12, 11]\n",
    "\n",
    "def get_feature_importance(rfc, X, Y):\n",
    "    rfc.fit(X, Y)\n",
    "    \n",
    "    # TODO: Graph each feature's importance to the model's accuracy\n",
    "    importances = rfc.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    for f in range(X.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices], color=\"orange\", align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), indices)\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def get_pca_cross_val_score(num_components):\n",
    "    X = df.drop('loan_status', 1)\n",
    "    X = pd.get_dummies(X)\n",
    "    old_num_cols = len(X.columns)\n",
    "    X = X.dropna(axis=1)\n",
    "    print('Dropped', old_num_cols - len(X.columns), 'columns')\n",
    "    print('Now have', len(X.columns), 'columns')\n",
    "    \n",
    "    pca = PCA(n_components=num_components)\n",
    "    principal_components = pca.fit_transform(X)\n",
    "    \n",
    "    rfc = ensemble.RandomForestClassifier()\n",
    "    Y = df['loan_status']\n",
    "    X = principal_components\n",
    "    \n",
    "    print(num_components, cross_val_score(rfc, X, Y, cv=10))\n",
    "    get_feature_importance(rfc, X, Y)\n",
    "\n",
    "    \n",
    "for num_components in pca_num_components:\n",
    "    get_pca_cross_val_score(num_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you do it without using anything related to payment amount or outstanding principal? How do you know?\n",
    "\n",
    "I can do this without anything related to payment amount or outstanding principal – I ran PCA again without those columns, and got extremely similar cross validation scores for the same number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_cross_val_score_wo_pymnt_or_principal(num_components):\n",
    "    X = df.drop('loan_status', 1)\n",
    "    X = X.drop('total_pymnt', 1)\n",
    "    X = X.drop('out_prncp', 1)\n",
    "    X = X.drop('total_pymnt_inv', 1)\n",
    "    X = X.drop('out_prncp_inv', 1)\n",
    "    X = pd.get_dummies(X)\n",
    "    old_num_cols = len(X.columns)\n",
    "    X = X.dropna(axis=1)\n",
    "    print('Dropped', old_num_cols - len(X.columns), 'columns')\n",
    "    print('Now have', len(X.columns), 'columns')\n",
    "    \n",
    "    pca = PCA(n_components=num_components)\n",
    "    principal_components = pca.fit_transform(X)\n",
    "    \n",
    "    rfc = ensemble.RandomForestClassifier()\n",
    "    Y = df['loan_status']\n",
    "    X = principal_components\n",
    "    \n",
    "    print(num_components, cross_val_score(rfc, X, Y, cv=10))\n",
    "    get_feature_importance(rfc, X, Y)\n",
    "\n",
    "for num_components in pca_num_components:\n",
    "    get_pca_cross_val_score_wo_pymnt_or_principal(num_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like 12 is the minimum number of components needed for PCA to still get > 90% cross validation scores across 10 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEEDBACK NOTES:\n",
    "\n",
    "# REMINDER: correlation matrices do not work for categorical (so no point in doing these correlation matrices)\n",
    "# May just show that I need to do PCA to get rid of unnecessary features that are highly correlated with others\n",
    "\n",
    "# you CAN test for multicollinearity, but doesn't matter for random forest\n",
    "# random forest is NOT affected by multicollinearity \n",
    "# (will simply vote for whatever the 2 correlated features say target should be)\n",
    "\n",
    "# To test effectiveness on categorical target:\n",
    "\n",
    "# T test will help you see:\n",
    "# - distribution of data for different categories is significantly different? T-test\n",
    "# - difference in categories' averages? (using standard deviation as well)\n",
    "\n",
    "# effective of category on category - chi square (REVIEW)\n",
    "# effect of category on continuous - t-test / z-test\n",
    "\n",
    "# random forest (ensemble model) – each decision tree votes on response / target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
