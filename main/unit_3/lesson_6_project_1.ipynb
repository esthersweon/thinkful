{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud\n",
    "\n",
    "Develop an algorithm to predict fraud. Prioritize correctly finding fraud rather than correctly labeling non-fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "raw_data = pd.read_csv('./data/creditcard.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frauds: 492\n",
      "Not Frauds: 284315\n",
      "Baseline R-squared: 0.9982725143693799\n"
     ]
    }
   ],
   "source": [
    "frauds = raw_data[raw_data['Class'] == 1]\n",
    "okays = raw_data[raw_data['Class'] == 0]\n",
    "\n",
    "baseline = 1 - len(frauds) / (len(frauds) + len(okays))\n",
    "print('Frauds:', len(frauds))\n",
    "print('Not Frauds:', len(okays))\n",
    "print('Baseline R-squared:', baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,10))\n",
    "\n",
    "# plt.subplot(2, 2, 1)\n",
    "# plt.title('Fraud Amounts')\n",
    "# plt.hist(frauds['Amount'], bins=20, label='Fraud', color='blue')\n",
    "\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.title('Okay Amounts')\n",
    "# plt.hist(okays['Amount'], bins=20, label='Okay', color='orange')\n",
    "\n",
    "# plt.subplot(2, 2, 3)\n",
    "# plt.title('Fraud Times')\n",
    "# plt.hist(frauds['Time'], bins=20, label='Fraud', color='blue')\n",
    "\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.title('Okay Times')\n",
    "# plt.hist(okays['Time'], bins=20, label='Okay', color='orange')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: \n",
    "# # Try taking out outliers – see distribution of remaining amounts\n",
    "# # Compare model with / without outliers\n",
    "\n",
    "# plt.figure(figsize=(20,10))\n",
    "\n",
    "# plt.subplot(2, 2, 1)\n",
    "# plt.title('Fraud Amounts')\n",
    "# plt.boxplot(frauds['Amount'])\n",
    "\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.title('Okay Amounts')\n",
    "# plt.boxplot(okays['Amount'])\n",
    "\n",
    "# plt.subplot(2, 2, 3)\n",
    "# plt.title('Fraud Times')\n",
    "# plt.boxplot(frauds['Time'])\n",
    "\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.title('Okay Times')\n",
    "# plt.boxplot(okays['Time'])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,60))\n",
    "\n",
    "# for i in range(28):\n",
    "#     col_name = 'V' + str(i + 1)\n",
    "    \n",
    "#     plt.subplot(28, 2, 2 * i + 1)\n",
    "#     plt.title('Frauds ' + col_name)\n",
    "#     plt.hist(frauds[col_name], bins=20, color='blue')\n",
    "    \n",
    "#     plt.subplot(28, 2, 2 * i + 2)\n",
    "#     plt.title('Okays ' + col_name)\n",
    "#     plt.hist(okays[col_name], bins=20, color='orange')\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,90))\n",
    "\n",
    "# for i in range(28):\n",
    "#     col_name = 'V' + str(i + 1)\n",
    "    \n",
    "#     plt.subplot(28, 2, 2 * i + 1)\n",
    "#     plt.title('Frauds ' + col_name)\n",
    "#     plt.boxplot(frauds[col_name])\n",
    "    \n",
    "#     plt.subplot(28, 2, 2 * i + 2)\n",
    "#     plt.title('Okays ' + col_name)\n",
    "#     plt.boxplot(okays[col_name])\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am looking to minimize Type II errors (\"false negative\"s – i.e. considering something that is a fraud as okay). But before I begin to build models, I need to make sure my data is not skewed by either undersampling my majority group (okays) or oversampling my minority group (frauds). \n",
    "\n",
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method for fitting and training model, returns formatted result\n",
    "def fit_and_train(model, fit_X_train, fit_Y_train, X_train, Y_train):\n",
    "    model_fit = model.fit(fit_X_train, fit_Y_train)\n",
    "    model_score_train = model.score(X_train, Y_train)\n",
    "    print('R² for train:', model_score_train)\n",
    "    \n",
    "    model_score_test = model.score(X_test, Y_test)\n",
    "    print('\\nR² for test:', model_score_test)\n",
    "    \n",
    "    model_improve_over_baseline = (model_score_test - baseline) / baseline\n",
    "    print('Improvement over baseline:', model_improve_over_baseline)\n",
    "    \n",
    "#     if hasattr(model_fit, 'coef_'):\n",
    "#         print('\\nCoefficients:', model_fit.coef_)\n",
    "    \n",
    "#     if hasattr(model_fit, 'intercept_'):\n",
    "#         print('\\nIntercept:', model_fit.intercept_)\n",
    "    \n",
    "#     if hasattr(X_train, 'columns'):\n",
    "#         print('Data cols:', list(X_train.columns))\n",
    "\n",
    "# Helper method for evaluating model, returns formatted result\n",
    "def evaluate_model_printout(model, train, test):\n",
    "    Y_train_vals = train['Class'].values\n",
    "    Y_test_vals = test['Class'].values\n",
    "    \n",
    "    predict_train = model.predict_proba(X_train)\n",
    "    predict_train = list(map(lambda x: 0 if x[0] > .998 else 1, predict_train))\n",
    "    predict_train = np.fromiter(predict_train, dtype=np.int)\n",
    "\n",
    "    predict_test = model.predict_proba(X_test)\n",
    "    predict_test = list(map(lambda x: 0 if x[0] > .998 else 1, predict_test))\n",
    "    predict_test = np.fromiter(predict_test, dtype=np.int)\n",
    "    \n",
    "    predict_whole = model.predict_proba(X)\n",
    "    predict_whole = list(map(lambda x: 0 if x[0] > .998 else 1, predict_whole))\n",
    "    predict_whole = np.fromiter(predict_whole, dtype=np.int)\n",
    "    \n",
    "    crosstab_labels = [0, 1, 'All']\n",
    "    table_train = pd.crosstab(Y_train_vals, predict_train, rownames=['actual'], colnames=['predicted'], margins=True)\n",
    "    table_train = table_train.reindex(index=crosstab_labels,columns=crosstab_labels, fill_value=0)\n",
    "\n",
    "    print('TRAIN:')\n",
    "#     print(table_train, '\\n')\n",
    "\n",
    "    train_tI_errors = table_train.loc[0,1] / table_train.loc['All','All']\n",
    "    train_tII_errors = table_train.loc[1,0] / table_train.loc['All','All']\n",
    "    print(('Accuracy:\\n% Type I errors: {}\\n% Type II errors: {}\\n').format(train_tI_errors, train_tII_errors))\n",
    "\n",
    "    train_precision = table_train.loc[1,1] / table_train.loc['All', 1] # correctly predicted positives / all predicted positives\n",
    "    train_recall = table_train.loc[1,1] / table_train.loc[1,'All'] # true positives / (true positives + false negatives)\n",
    "    print('Precision:', train_precision)\n",
    "    print('Recall:', train_recall, '\\n\\n----------\\n')\n",
    "    \n",
    "    print('TEST:')\n",
    "#     print(table_test, '\\n')\n",
    "\n",
    "    table_test = pd.crosstab(Y_test_vals, predict_test, rownames=['actual'], colnames=['predicted'], margins=True)\n",
    "    table_test = table_test.reindex(index=crosstab_labels,columns=crosstab_labels, fill_value=0)\n",
    "\n",
    "    test_tI_errors = table_test.loc[0,1]/table_test.loc['All','All']\n",
    "    test_tII_errors = table_test.loc[1,0]/table_test.loc['All','All']\n",
    "    print(('Accuracy:\\n% Type I errors: {}\\n% Type II errors: {}\\n').format(test_tI_errors, test_tII_errors))\n",
    "\n",
    "    test_precision = table_test.loc[1,1] / table_test.loc['All', 1] # correctly predicted positives / all predicted positives\n",
    "    test_recall = table_test.loc[1,1] / table_test.loc[1,'All'] # true positives / (true positives + false negatives)\n",
    "    print('Precision:', test_precision)\n",
    "    print('Recall:', test_recall)\n",
    "    \n",
    "    print('WHOLE DATASET:')\n",
    "    table_whole = pd.crosstab(Y, predict_whole, rownames=['actual'], colnames=['predicted'], margins=True)\n",
    "    table_whole = table_whole.reindex(index=crosstab_labels,columns=crosstab_labels, fill_value=0)\n",
    "    \n",
    "    whole_tI_errors = table_whole.loc[0,1]/table_whole.loc['All','All']\n",
    "    whole_tII_errors = table_whole.loc[1,0]/table_whole.loc['All','All']\n",
    "    print(('Accuracy:\\n% Type I errors: {}\\n% Type II errors: {}\\n').format(whole_tI_errors, whole_tII_errors))\n",
    "\n",
    "    whole_precision = table_whole.loc[1,1] / table_whole.loc['All', 1] # correctly predicted positives / all predicted positives\n",
    "    whole_recall = table_whole.loc[1,1] / table_whole.loc[1,'All'] # true positives / (true positives + false negatives)\n",
    "    print('Precision:', whole_precision)\n",
    "    print('Recall:', whole_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRATEGY 1: Undersampling Okays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly sample 492 (# of frauds) rows from okays (3 times)\n",
      "*** GRADIENT BOOSTING CLASSIFIER 1 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for train: 1.0\n",
      "\n",
      "R² for test: 0.9451219512195121\n",
      "Improvement over baseline: -0.05324253887070464\n",
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.20934959349593496\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.7048710601719198\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "Accuracy:\n",
      "% Type I errors: 0.25\n",
      "% Type II errors: 0.006097560975609756\n",
      "\n",
      "Precision: 0.6639344262295082\n",
      "Recall: 0.9878048780487805\n",
      "WHOLE DATASET:\n",
      "Accuracy:\n",
      "% Type I errors: 0.22967479674796748\n",
      "% Type II errors: 0.003048780487804878\n",
      "\n",
      "Precision: 0.6839160839160839\n",
      "Recall: 0.9939024390243902\n",
      "*** RANDOM FOREST 1 ***\n",
      "R² for train: 0.9878048780487805\n",
      "\n",
      "R² for test: 0.9369918699186992\n",
      "Improvement over baseline: -0.061386689073967333\n",
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.16666666666666666\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.75\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "% Type I errors: 0.258130081300813\n",
      "% Type II errors: 0.006097560975609756\n",
      "\n",
      "Precision: 0.6567567567567567\n",
      "Recall: 0.9878048780487805\n",
      "WHOLE DATASET:\n",
      "Accuracy:\n",
      "% Type I errors: 0.21239837398373984\n",
      "% Type II errors: 0.003048780487804878\n",
      "\n",
      "Precision: 0.7005730659025788\n",
      "Recall: 0.9939024390243902\n",
      "*** GRADIENT BOOSTING CLASSIFIER 2 ***\n",
      "R² for train: 1.0\n",
      "\n",
      "R² for test: 0.9349593495934959\n",
      "Improvement over baseline: -0.06342272662478306\n",
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.10772357723577236\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.822742474916388\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "Accuracy:\n",
      "% Type I errors: 0.18699186991869918\n",
      "% Type II errors: 0.006097560975609756\n",
      "\n",
      "Precision: 0.7253731343283583\n",
      "Recall: 0.9878048780487805\n",
      "WHOLE DATASET:\n",
      "Accuracy:\n",
      "% Type I errors: 0.14735772357723578\n",
      "% Type II errors: 0.003048780487804878\n",
      "\n",
      "Precision: 0.7712933753943217\n",
      "Recall: 0.9939024390243902\n",
      "*** RANDOM FOREST 2 ***\n",
      "R² for train: 0.9857723577235772\n",
      "\n",
      "R² for test: 0.9369918699186992\n",
      "Improvement over baseline: -0.061386689073967333\n",
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.16056910569105692\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.7569230769230769\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "Accuracy:\n",
      "% Type I errors: 0.2764227642276423\n",
      "% Type II errors: 0.006097560975609756\n",
      "\n",
      "Precision: 0.6411609498680739\n",
      "Recall: 0.9878048780487805\n",
      "WHOLE DATASET:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "% Type I errors: 0.2184959349593496\n",
      "% Type II errors: 0.003048780487804878\n",
      "\n",
      "Precision: 0.6946022727272727\n",
      "Recall: 0.9939024390243902\n",
      "*** GRADIENT BOOSTING CLASSIFIER 3 ***\n",
      "R² for train: 1.0\n",
      "\n",
      "R² for test: 0.9329268292682927\n",
      "Improvement over baseline: -0.06545876417559868\n",
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.10569105691056911\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.825503355704698\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "Accuracy:\n",
      "% Type I errors: 0.21138211382113822\n",
      "% Type II errors: 0.006097560975609756\n",
      "\n",
      "Precision: 0.7002881844380403\n",
      "Recall: 0.9878048780487805\n",
      "WHOLE DATASET:\n",
      "Accuracy:\n",
      "% Type I errors: 0.15853658536585366\n",
      "% Type II errors: 0.003048780487804878\n",
      "\n",
      "Precision: 0.7581395348837209\n",
      "Recall: 0.9939024390243902\n",
      "*** RANDOM FOREST 3 ***\n",
      "R² for train: 0.9979674796747967\n",
      "\n",
      "R² for test: 0.9390243902439024\n",
      "Improvement over baseline: -0.059350651523151714\n",
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.1402439024390244\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.780952380952381\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "Accuracy:\n",
      "% Type I errors: 0.2845528455284553\n",
      "% Type II errors: 0.006097560975609756\n",
      "\n",
      "Precision: 0.6344647519582245\n",
      "Recall: 0.9878048780487805\n",
      "WHOLE DATASET:\n",
      "Accuracy:\n",
      "% Type I errors: 0.21239837398373984\n",
      "% Type II errors: 0.003048780487804878\n",
      "\n",
      "Precision: 0.7005730659025788\n",
      "Recall: 0.9939024390243902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print('Randomly sample', len(frauds), '(# of frauds) rows from okays (3 times)')\n",
    "all_undersampled_okays = []\n",
    "\n",
    "for time in range(3):\n",
    "    rand_sample = okays.sample(n=len(frauds))\n",
    "    all_undersampled_okays.append(rand_sample)\n",
    "\n",
    "for idx, sample in enumerate(all_undersampled_okays):\n",
    "    df = pd.concat([frauds, sample])\n",
    "    df = df.reset_index()\n",
    "    df_test = df.iloc[::2]\n",
    "    df_train = df.iloc[1::2]\n",
    "\n",
    "    # TRAINING\n",
    "    X_train = df_train.loc[:, ~(df_train.columns).isin(['Class', 'index', 'Time', 'Amount'])]\n",
    "    Y_train = df_train['Class'].values.reshape(-1, 1)\n",
    "\n",
    "    # TESTING\n",
    "    X_test = df_test.loc[:, ~(df_train.columns).isin(['Class', 'index', 'Time', 'Amount'])]\n",
    "    Y_test = df_test['Class'].values.reshape(-1, 1)\n",
    "    \n",
    "    # WHOLE DATASET\n",
    "    X = df.loc[:, ~(df.columns).isin(['Class', 'index', 'Time', 'Amount'])]\n",
    "    Y = df['Class'].values\n",
    "\n",
    "#     # LASSO\n",
    "#     lasso = linear_model.LogisticRegression(penalty='l1', C=100) \n",
    "#     fit_and_train(lasso, X_train, Y_train, X_train, Y_train)\n",
    "#     evaluate_model_printout(lasso)\n",
    "    \n",
    "#     # RIDGE\n",
    "#     ridge = linear_model.LogisticRegression(penalty='l2', C=100, fit_intercept=False)\n",
    "#     fit_and_train(ridge, X_train, Y_train, X_train, Y_train)\n",
    "#     evaluate_model_printout(ridge)\n",
    "    \n",
    "#     # SVC\n",
    "#     svm = SVC(kernel='linear', probability=True)\n",
    "#     fit_and_train(svm, X_train, Y_train, X_train, Y_train)\n",
    "\n",
    "#     # Naive Bayes\n",
    "#     bnb = BernoulliNB()\n",
    "#     fit_and_train(bnb, X_train, Y_train, X_train, Y_train)\n",
    "\n",
    "    print('*** GRADIENT BOOSTING CLASSIFIER', idx + 1, '***')\n",
    "    gbm = ensemble.GradientBoostingClassifier(n_estimators=500, max_depth=2, loss='deviance')\n",
    "    fit_and_train(gbm, X_train, Y_train, X_train, Y_train)\n",
    "    evaluate_model_printout(gbm, df_train, df_test)\n",
    "    \n",
    "    print('*** RANDOM FOREST', idx + 1, '***')\n",
    "    rfc = ensemble.RandomForestClassifier()\n",
    "    fit_and_train(rfc, X_train, Y_train, X_train, Y_train)\n",
    "    evaluate_model_printout(rfc, df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am looking to minimize Type II errors (\"false negative\"s – i.e. considering something that is a fraud as okay). My test set gave 0.0% for Type II errors, which I am very happy with, especially since my percentage of Type I errors is also extremely low (~0.000035%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Could use ROC / AUC threshold for less performant classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on models that work to reduce \n",
    "# Find out if multicollinearity using LASSO – remove features to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRATEGY 2: Over-Sampling Frauds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "df_test = raw_data.iloc[::2]\n",
    "df_train = raw_data.iloc[1::2]\n",
    "\n",
    "X_train = df_train.loc[:, ~(df_train.columns).isin(['Class'])]\n",
    "Y_train = df_train['Class'].values.reshape(-1, 1)\n",
    "\n",
    "# TESTING\n",
    "X_test = df_test.loc[:, ~(df_test.columns).isin(['Class'])]\n",
    "Y_test = df_test['Class'].values.reshape(-1, 1)\n",
    "\n",
    "sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "X_train_res, Y_train_res = sm.fit_sample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "1   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "2   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "3   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "4   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "1  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "2  0.260314 -0.568671  ...   -0.208254 -0.559825 -0.026398 -0.371427   \n",
       "3 -3.807864  0.615375  ...    1.943465 -1.015455  0.057504 -0.649709   \n",
       "4  0.069539 -0.736727  ...   -0.246914 -0.633753 -0.120794 -0.385050   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "1  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "2 -0.232794  0.105915  0.253844  0.081080    3.67      0  \n",
       "3 -0.415267 -0.051634 -1.206921 -1.085339   40.80      0  \n",
       "4 -0.069733  0.094199  0.246219  0.083076    3.68      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_df = pd.DataFrame(X_train_res, columns=df.columns[1:-1])\n",
    "oversampled_df['Class'] = Y_train_res\n",
    "oversampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_train = oversampled_df.iloc[::2]\n",
    "oversampled_test = oversampled_df.iloc[1::2]\n",
    "\n",
    "X_train = oversampled_train.loc[:, ~(oversampled_train.columns).isin(['Class'])]\n",
    "Y_train = oversampled_train['Class'].values.reshape(-1, 1)\n",
    "\n",
    "X_test = oversampled_test.loc[:, ~(oversampled_test.columns).isin(['Class'])]\n",
    "Y_test = oversampled_test['Class'].values.reshape(-1, 1)\n",
    "\n",
    "X = oversampled_df.loc[:, ~(oversampled_df.columns).isin(['Class'])]\n",
    "Y = oversampled_df['Class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for train: 0.9778070089053333\n",
      "\n",
      "R² for test: 0.9778070089053333\n",
      "Improvement over baseline: -0.020500920509641463\n"
     ]
    }
   ],
   "source": [
    "lasso = linear_model.LogisticRegression(penalty='l1', C=100) \n",
    "fit_and_train(lasso, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating LASSO Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.4450064011479861\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.5291039554582943\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "Accuracy:\n",
      "% Type I errors: 0.44400050646445605\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.5296537232021103\n",
      "Recall: 1.0\n",
      "WHOLE DATASET:\n",
      "Accuracy:\n",
      "% Type I errors: 0.4445034538062211\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.52937868886034\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_printout(lasso, oversampled_train, oversampled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for train: 0.961100716084467\n",
      "\n",
      "R² for test: 0.961100716084467\n",
      "Improvement over baseline: -0.03723612315259902\n"
     ]
    }
   ],
   "source": [
    "ridge = linear_model.LogisticRegression(penalty='l2', C=100, fit_intercept=False)\n",
    "fit_and_train(ridge, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Ridge Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.4999859315428877\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.5000140684571123\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "Accuracy:\n",
      "% Type I errors: 0.5000140684571123\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.4999859315428877\n",
      "Recall: 1.0\n",
      "WHOLE DATASET:\n",
      "Accuracy:\n",
      "% Type I errors: 0.5\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.5\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_printout(ridge, oversampled_train, oversampled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for train: 0.9986001885173253\n",
      "\n",
      "R² for test: 0.9986001885173253\n",
      "Improvement over baseline: 0.0003282411798670318\n"
     ]
    }
   ],
   "source": [
    "gbm = ensemble.GradientBoostingClassifier(n_estimators=500, max_depth=2, loss='deviance')\n",
    "fit_and_train(gbm, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.2208185028347941\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.6936618687484752\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "Accuracy:\n",
      "% Type I errors: 0.21974930009425866\n",
      "% Type II errors: 7.0342285561542466e-06\n",
      "\n",
      "Precision: 0.6946773783693974\n",
      "Recall: 0.9999859311470336\n",
      "WHOLE DATASET:\n",
      "Accuracy:\n",
      "% Type I errors: 0.2202839014645264\n",
      "% Type II errors: 3.5171142780771233e-06\n",
      "\n",
      "Precision: 0.6941692343000004\n",
      "Recall: 0.9999929657714438\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_printout(gbm, oversampled_train, oversampled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <s>Support Vector Machine</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = SVC(kernel='linear', probability=True)\n",
    "# fit_and_train(svm, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_printout(svm, oversampled_train, oversampled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for train: 0.999866349657433\n",
      "\n",
      "R² for test: 0.999866349657433\n",
      "Improvement over baseline: 0.0015965933801753208\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "fit_and_train(rfc, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.008638032666957415\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.9830177981220009\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "Accuracy:\n",
      "% Type I errors: 0.020582152755307326\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.9604621309370989\n",
      "Recall: 1.0\n",
      "WHOLE DATASET:\n",
      "Accuracy:\n",
      "% Type I errors: 0.01461009271113237\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.9716093933677793\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_printout(rfc, oversampled_train, oversampled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for train: 0.9448235112055261\n",
      "\n",
      "R² for test: 0.9448235112055261\n",
      "Improvement over baseline: -0.053541495327674285\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "fit_and_train(bnb, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.0340386319832304\n",
      "% Type II errors: 0.024802689888999873\n",
      "\n",
      "Precision: 0.9331592905685397\n",
      "Recall: 0.9503960159250454 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "Accuracy:\n",
      "% Type I errors: 0.033532167527187295\n",
      "% Type II errors: 0.02408519857627214\n",
      "\n",
      "Precision: 0.9341774598878794\n",
      "Recall: 0.951828247442986\n",
      "WHOLE DATASET:\n",
      "Accuracy:\n",
      "% Type I errors: 0.03378539975520885\n",
      "% Type II errors: 0.024443944232636006\n",
      "\n",
      "Precision: 0.9336684666270767\n",
      "Recall: 0.951112111534728\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_printout(bnb, oversampled_train, oversampled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Oversampling the minority class (frauds) proved a better strategy to deal with the skewed dataset than undersampling the majority class (not frauds), as the models built after oversampling were more accurate.\n",
    "\n",
    "Of all the classifiers built using oversampling, the Random Forest Classifier was the most performant, as it 1) minimized errors, especially Type II errors (Type I - 1.46%, Type II - 0%), and 2) showed the highest precision (97.16%) and recall (100%) rate. For a problem like this, we are more concerned with a high recall rate than precision rate, though both are important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MENTOR NOTES:\n",
    "# Precision – % of my + predictions that are correct\n",
    "# Recall – % of my target +s I predicted\n",
    "\n",
    "# CURRICULUM NOTES:\n",
    "# There are a few things you can do to deal with class imbalance:\n",
    "\n",
    "# Ignore. If we really only care about the absolute accuracy of the model and our sample is representative of the population, \n",
    "# this can be a reasonable strategy. Engineer features that strongly identify the minority class, and this can turn out ok.\n",
    "\n",
    "# Change your sampling. If you oversample the minority class or undersample the majority class, you can create a more balanced training set. \n",
    "# This is particularly useful if the goal of your model is to correctly identify the minority class. This can also be done by creating synthetic samples \n",
    "# to try to make your data more balanced or weighting samples to balance out your classes. \n",
    "\n",
    "# Probability outputs. \n",
    "# Although Naive Bayes' probability outputs are generally inaccurate, other models will give you a more accurate probability of a certain class. \n",
    "# e.g. logistic regression or support vector machines (SVM)\n",
    "# Instead of just taking the most likely outcome, you can set up a specific cutoff or a more complex rule. \n",
    "# In the binary case, it could be going with the minority case if it has a priority greater than some threshold.\n",
    "\n",
    "# Lastly, you can create cost functions for errors. This quantifies ways in which errors are not equal – scale the cost of an error up or down. \n",
    "# This can mean something like a Type II error being twice as bad as a Type I error, or however you choose to quantify that relationship. \n",
    "# SKLearn's Naive Bayes model does not have an easy built-in way to do this, but it's a good thing to keep in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
