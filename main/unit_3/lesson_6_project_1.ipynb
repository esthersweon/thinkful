{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud\n",
    "\n",
    "Develop an algorithm to predict fraud. Prioritize correctly finding fraud rather than correctly labeling non-fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "raw_data = pd.read_csv('./data/creditcard.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frauds: 492\n",
      "Not Frauds: 284315\n",
      "Baseline R-squared: 0.9982725143693799\n"
     ]
    }
   ],
   "source": [
    "frauds = raw_data[raw_data['Class'] == 1]\n",
    "okays = raw_data[raw_data['Class'] == 0]\n",
    "\n",
    "baseline = 1 - len(frauds) / (len(frauds) + len(okays))\n",
    "print('Frauds:', len(frauds))\n",
    "print('Not Frauds:', len(okays))\n",
    "print('Baseline R-squared:', baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,10))\n",
    "\n",
    "# plt.subplot(2, 2, 1)\n",
    "# plt.title('Fraud Amounts')\n",
    "# plt.hist(frauds['Amount'], bins=20, label='Fraud', color='blue')\n",
    "\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.title('Okay Amounts')\n",
    "# plt.hist(okays['Amount'], bins=20, label='Okay', color='orange')\n",
    "\n",
    "# plt.subplot(2, 2, 3)\n",
    "# plt.title('Fraud Times')\n",
    "# plt.hist(frauds['Time'], bins=20, label='Fraud', color='blue')\n",
    "\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.title('Okay Times')\n",
    "# plt.hist(okays['Time'], bins=20, label='Okay', color='orange')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: \n",
    "# # Try taking out outliers – see distribution of remaining amounts\n",
    "# # Compare model with / without outliers\n",
    "\n",
    "# plt.figure(figsize=(20,10))\n",
    "\n",
    "# plt.subplot(2, 2, 1)\n",
    "# plt.title('Fraud Amounts')\n",
    "# plt.boxplot(frauds['Amount'])\n",
    "\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.title('Okay Amounts')\n",
    "# plt.boxplot(okays['Amount'])\n",
    "\n",
    "# plt.subplot(2, 2, 3)\n",
    "# plt.title('Fraud Times')\n",
    "# plt.boxplot(frauds['Time'])\n",
    "\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.title('Okay Times')\n",
    "# plt.boxplot(okays['Time'])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,60))\n",
    "\n",
    "# for i in range(28):\n",
    "#     col_name = 'V' + str(i + 1)\n",
    "    \n",
    "#     plt.subplot(28, 2, 2 * i + 1)\n",
    "#     plt.title('Frauds ' + col_name)\n",
    "#     plt.hist(frauds[col_name], bins=20, color='blue')\n",
    "    \n",
    "#     plt.subplot(28, 2, 2 * i + 2)\n",
    "#     plt.title('Okays ' + col_name)\n",
    "#     plt.hist(okays[col_name], bins=20, color='orange')\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,90))\n",
    "\n",
    "# for i in range(28):\n",
    "#     col_name = 'V' + str(i + 1)\n",
    "    \n",
    "#     plt.subplot(28, 2, 2 * i + 1)\n",
    "#     plt.title('Frauds ' + col_name)\n",
    "#     plt.boxplot(frauds[col_name])\n",
    "    \n",
    "#     plt.subplot(28, 2, 2 * i + 2)\n",
    "#     plt.title('Okays ' + col_name)\n",
    "#     plt.boxplot(okays[col_name])\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am looking to minimize Type II errors (\"false negative\"s – i.e. considering something that is a fraud as okay). But before I begin to build models, I need to make sure my data is not skewed by either undersampling my majority group (okays) or oversampling my minority group (frauds). \n",
    "\n",
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method for fitting and training model, returns formatted result\n",
    "def fit_and_train(model, fit_X_train, fit_Y_train, X_train, Y_train):\n",
    "    model_fit = model.fit(fit_X_train, fit_Y_train)\n",
    "    model_score_train = model.score(X_train, Y_train)\n",
    "    print('R² for train:', model_score_train)\n",
    "    \n",
    "    model_score_test = model.score(X_test, Y_test)\n",
    "    print('\\nR² for test:', model_score_test)\n",
    "    \n",
    "    model_improve_over_baseline = (model_score_test - baseline) / baseline\n",
    "    print('Improvement over baseline:', model_improve_over_baseline)\n",
    "    \n",
    "#     if hasattr(model_fit, 'coef_'):\n",
    "#         print('\\nCoefficients:', model_fit.coef_)\n",
    "    \n",
    "#     if hasattr(model_fit, 'intercept_'):\n",
    "#         print('\\nIntercept:', model_fit.intercept_)\n",
    "    \n",
    "#     if hasattr(X_train, 'columns'):\n",
    "#         print('Data cols:', list(X_train.columns))\n",
    "\n",
    "# Helper method for evaluating model, returns formatted result\n",
    "def evaluate_model_printout(model):\n",
    "    Y_train_vals = df_train['Class'].values\n",
    "    Y_test_vals = df_test['Class'].values\n",
    "\n",
    "    predict_train = model.predict_proba(X_train)\n",
    "    predict_train = list(map(lambda x: 0 if x[0] > .998 else 1, predict_train))\n",
    "    predict_train = np.fromiter(predict_train, dtype=np.int)\n",
    "\n",
    "    predict_test = model.predict_proba(X_test)\n",
    "    predict_test = list(map(lambda x: 0 if x[0] > .998 else 1, predict_test))\n",
    "    predict_test = np.fromiter(predict_test, dtype=np.int)\n",
    "    \n",
    "    crosstab_labels = [0, 1, 'All']\n",
    "    table_train = pd.crosstab(Y_train_vals, predict_train, rownames=['actual'], colnames=['predicted'], margins=True)\n",
    "    table_train = table_train.reindex(index=crosstab_labels,columns=crosstab_labels, fill_value=0)\n",
    "\n",
    "    print('TRAIN:')\n",
    "#     print(table_train, '\\n')\n",
    "\n",
    "    train_tI_errors = table_train.loc[0,1] / table_train.loc['All','All']\n",
    "    train_tII_errors = table_train.loc[1,0] / table_train.loc['All','All']\n",
    "    print(('Accuracy:\\n% Type I errors: {}\\n% Type II errors: {}\\n').format(train_tI_errors, train_tII_errors))\n",
    "\n",
    "    train_precision = table_train.loc[1,1] / table_train.loc['All', 1] # correctly predicted positives / all predicted positives\n",
    "    train_recall = table_train.loc[1,1] / table_train.loc[1,'All'] # true positives / (true positives + false negatives)\n",
    "    print('Precision:', train_precision)\n",
    "    print('Recall:', train_recall, '\\n\\n----------\\n')\n",
    "\n",
    "    table_test = pd.crosstab(Y_test_vals, predict_test, rownames=['actual'], colnames=['predicted'], margins=True)\n",
    "    table_test = table_test.reindex(index=crosstab_labels,columns=crosstab_labels, fill_value=0)\n",
    "    \n",
    "    print('TEST:')\n",
    "#     print(table_test, '\\n')\n",
    "\n",
    "    test_tI_errors = table_test.loc[0,1]/table_test.loc['All','All']\n",
    "    test_tII_errors = table_test.loc[1,0]/table_test.loc['All','All']\n",
    "    print(('Accuracy:\\n% Type I errors: {}\\n% Type II errors: {}\\n').format(test_tI_errors, test_tII_errors))\n",
    "\n",
    "    test_precision = table_test.loc[1,1] / table_test.loc['All', 1] # correctly predicted positives / all predicted positives\n",
    "    test_recall = table_test.loc[1,1] / table_test.loc[1,'All'] # true positives / (true positives + false negatives)\n",
    "    print('Precision:', test_precision)\n",
    "    print('Recall:', test_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRATEGY 1: Undersampling Okays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly sample 492 (# of frauds) rows from okays (3 times)\n",
      "*** GRADIENT BOOSTING CLASSIFIER 1 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for train: 1.0\n",
      "\n",
      "R² for test: 0.9329268292682927\n",
      "Improvement over baseline: -0.06545876417559868\n",
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.2032520325203252\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.7109826589595376\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "predicted    0    1  All\n",
      "actual                  \n",
      "0          115  131  246\n",
      "1            1  245  246\n",
      "All        116  376  492 \n",
      "\n",
      "Accuracy:\n",
      "% Type I errors: 0.266260162601626\n",
      "% Type II errors: 0.0020325203252032522\n",
      "\n",
      "Precision: 0.651595744680851\n",
      "Recall: 0.9959349593495935\n",
      "*** RANDOM FOREST 1 ***\n",
      "R² for train: 0.9817073170731707\n",
      "\n",
      "R² for test: 0.9349593495934959\n",
      "Improvement over baseline: -0.06342272662478306\n",
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.15853658536585366\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.7592592592592593\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "predicted    0    1  All\n",
      "actual                  \n",
      "0          108  138  246\n",
      "1            4  242  246\n",
      "All        112  380  492 \n",
      "\n",
      "Accuracy:\n",
      "% Type I errors: 0.2804878048780488\n",
      "% Type II errors: 0.008130081300813009\n",
      "\n",
      "Precision: 0.6368421052631579\n",
      "Recall: 0.983739837398374\n",
      "*** GRADIENT BOOSTING CLASSIFIER 2 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for train: 1.0\n",
      "\n",
      "R² for test: 0.9451219512195121\n",
      "Improvement over baseline: -0.05324253887070464\n",
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.18089430894308944\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.7343283582089553\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "predicted    0    1  All\n",
      "actual                  \n",
      "0          136  110  246\n",
      "1            1  245  246\n",
      "All        137  355  492 \n",
      "\n",
      "Accuracy:\n",
      "% Type I errors: 0.22357723577235772\n",
      "% Type II errors: 0.0020325203252032522\n",
      "\n",
      "Precision: 0.6901408450704225\n",
      "Recall: 0.9959349593495935\n",
      "*** RANDOM FOREST 2 ***\n",
      "R² for train: 0.991869918699187\n",
      "\n",
      "R² for test: 0.9410569105691057\n",
      "Improvement over baseline: -0.05731461397233599\n",
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.18089430894308944\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.7343283582089553\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "predicted    0    1  All\n",
      "actual                  \n",
      "0          102  144  246\n",
      "1            2  244  246\n",
      "All        104  388  492 \n",
      "\n",
      "Accuracy:\n",
      "% Type I errors: 0.2926829268292683\n",
      "% Type II errors: 0.0040650406504065045\n",
      "\n",
      "Precision: 0.6288659793814433\n",
      "Recall: 0.991869918699187\n",
      "*** GRADIENT BOOSTING CLASSIFIER 3 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for train: 1.0\n",
      "\n",
      "R² for test: 0.9349593495934959\n",
      "Improvement over baseline: -0.06342272662478306\n",
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.16463414634146342\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.7522935779816514\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "predicted    0    1  All\n",
      "actual                  \n",
      "0          125  121  246\n",
      "1            0  246  246\n",
      "All        125  367  492 \n",
      "\n",
      "Accuracy:\n",
      "% Type I errors: 0.2459349593495935\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.670299727520436\n",
      "Recall: 1.0\n",
      "*** RANDOM FOREST 3 ***\n",
      "R² for train: 0.9939024390243902\n",
      "\n",
      "R² for test: 0.9532520325203252\n",
      "Improvement over baseline: -0.04509838866744184\n",
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.16666666666666666\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.75\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "predicted   0    1  All\n",
      "actual                 \n",
      "0          97  149  246\n",
      "1           2  244  246\n",
      "All        99  393  492 \n",
      "\n",
      "Accuracy:\n",
      "% Type I errors: 0.30284552845528456\n",
      "% Type II errors: 0.0040650406504065045\n",
      "\n",
      "Precision: 0.6208651399491094\n",
      "Recall: 0.991869918699187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print('Randomly sample', len(frauds), '(# of frauds) rows from okays (3 times)')\n",
    "all_undersampled_okays = []\n",
    "\n",
    "for time in range(3):\n",
    "    rand_sample = okays.sample(n=len(frauds))\n",
    "    all_undersampled_okays.append(rand_sample)\n",
    "\n",
    "for idx, sample in enumerate(all_undersampled_okays):\n",
    "    df = pd.concat([frauds, sample])\n",
    "    df = df.reset_index()\n",
    "    df_test = df.iloc[::2]\n",
    "    df_train = df.iloc[1::2]\n",
    "\n",
    "    # TRAINING\n",
    "    X_train = df_train.loc[:, ~(df_train.columns).isin(['Class', 'index', 'Time', 'Amount'])]\n",
    "    Y_train = df_train['Class'].values.reshape(-1, 1)\n",
    "\n",
    "    # TESTING\n",
    "    X_test = df_test.loc[:, ~(df_train.columns).isin(['Class', 'index', 'Time', 'Amount'])]\n",
    "    Y_test = df_test['Class'].values.reshape(-1, 1)\n",
    "\n",
    "#     # LASSO\n",
    "#     lasso = linear_model.LogisticRegression(penalty='l1', C=100) \n",
    "#     fit_and_train(lasso, X_train, Y_train, X_train, Y_train)\n",
    "#     evaluate_model_printout(lasso)\n",
    "    \n",
    "#     # RIDGE\n",
    "#     ridge = linear_model.LogisticRegression(penalty='l2', C=100, fit_intercept=False)\n",
    "#     fit_and_train(ridge, X_train, Y_train, X_train, Y_train)\n",
    "#     evaluate_model_printout(ridge)\n",
    "    \n",
    "#     # SVC\n",
    "#     svm = SVC(kernel='linear', probability=True)\n",
    "#     fit_and_train(svm, X_train, Y_train, X_train, Y_train)\n",
    "\n",
    "#     # Naive Bayes\n",
    "#     bnb = BernoulliNB()\n",
    "#     fit_and_train(bnb, X_train, Y_train, X_train, Y_train)\n",
    "\n",
    "    print('*** GRADIENT BOOSTING CLASSIFIER', idx + 1, '***')\n",
    "    gbm = ensemble.GradientBoostingClassifier(n_estimators=500, max_depth=2, loss='deviance')\n",
    "    fit_and_train(gbm, X_train, Y_train, X_train, Y_train)\n",
    "    evaluate_model_printout(gbm)\n",
    "    \n",
    "    print('*** RANDOM FOREST', idx + 1, '***')\n",
    "    rfc = ensemble.RandomForestClassifier()\n",
    "    fit_and_train(rfc, X_train, Y_train, X_train, Y_train)\n",
    "    evaluate_model_printout(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am looking to minimize Type II errors (\"false negative\"s – i.e. considering something that is a fraud as okay). My test set gave 0.0% for Type II errors, which I am very happy with, especially since my percentage of Type I errors is also extremely low (~0.000035%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Could use ROC / AUC threshold for less performant classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on models that work to reduce \n",
    "# Find out if multicollinearity using LASSO – remove features to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STRATEGY 2: Over-Sampling Frauds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "df_test = raw_data.iloc[::2]\n",
    "df_train = raw_data.iloc[1::2]\n",
    "\n",
    "X_train = df_train.loc[:, ~(df_train.columns).isin(['Class'])]\n",
    "Y_train = df_train['Class'].values.reshape(-1, 1)\n",
    "\n",
    "# TESTING\n",
    "X_test = df_test.loc[:, ~(df_train.columns).isin(['Class'])]\n",
    "Y_test = df_test['Class'].values.reshape(-1, 1)\n",
    "\n",
    "sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "X_train_res, Y_train_res = sm.fit_sample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for train: 0.992120952508023\n",
      "\n",
      "R² for test: 0.9922544310553074\n",
      "Improvement over baseline: -0.006028497456803355\n"
     ]
    }
   ],
   "source": [
    "lasso = linear_model.LogisticRegression(penalty='l1', C=100) \n",
    "fit_and_train(lasso, X_train_res, Y_train_res, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating LASSO Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.8685069837011861\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.0019448187929211823\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "predicted      0       1     All\n",
      "actual                          \n",
      "0          18496  123657  142153\n",
      "1              1     250     251\n",
      "All        18497  123907  142404 \n",
      "\n",
      "Accuracy:\n",
      "% Type I errors: 0.8683534170388473\n",
      "% Type II errors: 7.022274655206314e-06\n",
      "\n",
      "Precision: 0.0020176422639560316\n",
      "Recall: 0.9960159362549801\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_printout(lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for train: 0.9842348826920782\n",
      "\n",
      "R² for test: 0.9841366815538889\n",
      "Improvement over baseline: -0.014160294520801055\n"
     ]
    }
   ],
   "source": [
    "ridge = linear_model.LogisticRegression(penalty='l2', C=100, fit_intercept=False)\n",
    "fit_and_train(ridge, X_train_res, Y_train_res, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Ridge Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "Accuracy:\n",
      "% Type I errors: 0.9979845930212145\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.0016929269372071623\n",
      "Recall: 1.0 \n",
      "\n",
      "----------\n",
      "\n",
      "TEST:\n",
      "predicted   0       1     All\n",
      "actual                       \n",
      "0          61  142092  142153\n",
      "1           0     251     251\n",
      "All        61  142343  142404 \n",
      "\n",
      "Accuracy:\n",
      "% Type I errors: 0.9978090503075756\n",
      "% Type II errors: 0.0\n",
      "\n",
      "Precision: 0.0017633462832735013\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_printout(ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-88be6ef0a311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deviance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfit_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-f07181a0b68a>\u001b[0m in \u001b[0;36mfit_and_train\u001b[0;34m(model, fit_X_train, fit_Y_train, X_train, Y_train)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Helper method for fitting and training model, returns formatted result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_Y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_Y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_score_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'R² for train:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_score_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1034\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m--> 788\u001b[0;31m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gbm = ensemble.GradientBoostingClassifier(n_estimators=500, max_depth=2, loss='deviance')\n",
    "fit_and_train(gbm, X_train_res, Y_train_res, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_printout(gbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <s>Support Vector Machine</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = SVC(kernel='linear', probability=True)\n",
    "# fit_and_train(svm, X_train_res, Y_train_res, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model_printout(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "fit_and_train(rfc, X_train_res, Y_train_res, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_printout(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = BernoulliNB()\n",
    "fit_and_train(bnb, X_train_res, Y_train_res, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_printout(bnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Under-sampling the majority class (i.e. not frauds) proved a better strategy to deal with the skewed dataset than over-sampling the minority class (i.e. frauds), as the models built after under-sampling were much more accurate.\n",
    "\n",
    "Of all the classifiers built using under-sampling, the Naive Bayes model was the most performant, as it 1) minimized errors, especially Type II errors (Type I - 1~5%, Type II - ~2%), and 2) showed the highest precision (~75%) and recall (~95%) rate. For a problem like this, we are more concerned with a high recall rate than precision rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MENTOR NOTES:\n",
    "# Precision – % of my + predictions that are correct\n",
    "# Recall – % of my target +s I predicted\n",
    "\n",
    "# CURRICULUM NOTES:\n",
    "# There are a few things you can do to deal with class imbalance:\n",
    "\n",
    "# Ignore. If we really only care about the absolute accuracy of the model and our sample is representative of the population, \n",
    "# this can be a reasonable strategy. Engineer features that strongly identify the minority class, and this can turn out ok.\n",
    "\n",
    "# Change your sampling. If you oversample the minority class or undersample the majority class, you can create a more balanced training set. \n",
    "# This is particularly useful if the goal of your model is to correctly identify the minority class. This can also be done by creating synthetic samples \n",
    "# to try to make your data more balanced or weighting samples to balance out your classes. \n",
    "\n",
    "# Probability outputs. \n",
    "# Although Naive Bayes' probability outputs are generally inaccurate, other models will give you a more accurate probability of a certain class. \n",
    "# e.g. logistic regression or support vector machines (SVM)\n",
    "# Instead of just taking the most likely outcome, you can set up a specific cutoff or a more complex rule. \n",
    "# In the binary case, it could be going with the minority case if it has a priority greater than some threshold.\n",
    "\n",
    "# Lastly, you can create cost functions for errors. This quantifies ways in which errors are not equal – scale the cost of an error up or down. \n",
    "# This can mean something like a Type II error being twice as bad as a Type I error, or however you choose to quantify that relationship. \n",
    "# SKLearn's Naive Bayes model does not have an easy built-in way to do this, but it's a good thing to keep in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
