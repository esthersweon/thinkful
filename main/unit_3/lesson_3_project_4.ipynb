{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Pick a dataset of your choice with a binary outcome and the potential for at least 15 features.\n",
    "\n",
    "Engineer your features, then create 3 models. Each model will be run on a training set and a test-set (or multiple test-sets, if you take a folds approach):\n",
    "\n",
    "- Vanilla logistic regression\n",
    "- Ridge logistic regression\n",
    "- Lasso logistic regression\n",
    "\n",
    "If you're stuck on how to begin combining your 2 new modeling skills, here's a hint: the SKlearn LogisticRegression method has a \"penalty\" argument that takes either 'l1' or 'l2' as a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole weight</th>\n",
       "      <th>shucked weight</th>\n",
       "      <th>viscera weight</th>\n",
       "      <th>shell weight</th>\n",
       "      <th>rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sex  length  diameter  height  whole weight  shucked weight  viscera weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   shell weight  rings  \n",
       "0         0.150     15  \n",
       "1         0.070      7  \n",
       "2         0.210      9  \n",
       "3         0.155     10  \n",
       "4         0.055      7  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('./data/abalone.csv', names=[\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\", \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"])\n",
    "# all_sexes = df[\"sex\"]\n",
    "\n",
    "# Standardize data\n",
    "# names = list(df.columns)\n",
    "# names.remove(\"sex\")\n",
    "# df = pd.DataFrame(preprocessing.scale(df[names]), columns=names)\n",
    "# df[\"sex\"] = all_sexes\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df.shape[0]\n",
    "trainsize = int(num_rows / 2)\n",
    "df_test = df.iloc[trainsize:, :].copy()\n",
    "df_train = df.iloc[:trainsize, :].copy()\n",
    "\n",
    "# TRAINING\n",
    "X_train = df_train.loc[:, ~(df_train.columns).isin(['rings', 'sex'])]\n",
    "Y_train = df_train['rings'].values.reshape(-1, 1)\n",
    "\n",
    "# TESTING\n",
    "X_test = df_test.loc[:, ~(df_train.columns).isin(['rings', 'sex'])]\n",
    "Y_test = df_test['rings'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R²: 0.5287212733800659\n",
      "\n",
      "Coefficients: [[ -3.53448893  17.65380803   7.03000173  11.81187581 -22.71063649\n",
      "  -11.89651271   5.03184816]]\n",
      "\n",
      "Intercept: [2.92780683]\n"
     ]
    }
   ],
   "source": [
    "vanilla = linear_model.LinearRegression()\n",
    "vanilla.fit(X_train, Y_train)\n",
    "print('\\nR²:', vanilla.score(X_train, Y_train))\n",
    "print('\\nCoefficients:', vanilla.coef_)\n",
    "print('\\nIntercept:', vanilla.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R²: 0.5200804761044662\n"
     ]
    }
   ],
   "source": [
    "print('\\nR²:', vanilla.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge = linear_model(penalty=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.5112154373106311\n",
      "\n",
      "Coefficients: [[  8.01999961  12.76564296   7.70335814   8.02944719 -20.10108544\n",
      "   -9.01806838   6.69822391]]\n",
      "\n",
      "Intercept: 0.0\n"
     ]
    }
   ],
   "source": [
    "# coefficients will not go to zero\n",
    "# in the beginning, it is good to go with lasso because it will tell you which are the good variables to have\n",
    "\n",
    "# coefficients will change based on scale of data\n",
    "# if coefficient is 1-10, good alpha might be 5\n",
    "# if coefficient is less than 1, smaller alphas will be sufficient\n",
    "ridge = linear_model.Ridge(alpha=0.5, fit_intercept=False)\n",
    "ridgefit = ridge.fit(X_train, Y_train)\n",
    "\n",
    "print('R²:', ridge.score(X_train, Y_train))\n",
    "print('\\nCoefficients:', ridgefit.coef_)\n",
    "print('\\nIntercept:', ridgefit.intercept_)\n",
    "\n",
    "# try also with different data? good for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R²: 0.5085877627775135\n"
     ]
    }
   ],
   "source": [
    "print('\\nR²:', ridge.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso = linear_model(penalty=\"l1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.5278667911499331\n",
      "\n",
      "Coefficients: [ -0.          13.20574399   6.01743935  10.97734904 -21.98380604\n",
      " -10.1062783    5.88822553]\n",
      "\n",
      "Intercept: [2.93493665]\n"
     ]
    }
   ],
   "source": [
    "# More weight (alpha) --> more features will be zero (more you are penalizing)\n",
    "lasso = linear_model.Lasso(alpha=.001)\n",
    "lassofit = lasso.fit(X_train, Y_train)\n",
    "\n",
    "print('R²:', lasso.score(X_train, Y_train))\n",
    "print('\\nCoefficients:', lassofit.coef_)\n",
    "print('\\nIntercept:', lassofit.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R²: 0.5213568146382644\n"
     ]
    }
   ],
   "source": [
    "print('\\nR²:', lasso.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS / TIMELINE\n",
    "\n",
    "# do lasso\n",
    "# get coefficients to see which are zero\n",
    "# make sure R-squared values for train and test are similar\n",
    "# i do not have to take features out manually because coefficients will go to zero if they are unimportant\n",
    "\n",
    "# remove features from ridge, bc ridge will not make unimportant features zero\n",
    "# do ridge - will help prevent overfitting\n",
    "\n",
    "# REPEAT (trial and error) – look at data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models\n",
    "\n",
    "Evaluate all 3 models and decide on your best. Be clear about the decisions you made that led to these models (feature selection, regularization parameter selection, model evaluation criteria) and why you think that particular model is the best of the three. Also reflect on the strengths and limitations of regression as a modeling approach. Were there things you couldn't do but you wish you could have done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep features same\n",
    "# adapt alphas to be different for ridge vs. lasso\n",
    "\n",
    "# Evaluation criteria – R-squared for test and train\n",
    "\n",
    "# Independent and dependent variables are normally distributed (big assumption of regression)\n",
    "# If not normally distributed, regression will fail\n",
    "# Random forest / KNN – will work better in those scenarios (not limited by distribution)\n",
    "\n",
    "# RANDOM FOREST\n",
    "# random forest is most common model to use\n",
    "# add: gradient boosting machines to optimize ^\n",
    "# nice because you don't have to make variables normally distributed\n",
    "# scaling not 100% necessary for random forest\n",
    "# outlier removal and dummies and cleaning is still necessary for random forest\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "# more commonly used than linear regression\n",
    "# easy to explain \n",
    "\n",
    "# kaggle xgboost (https://www.kaggle.com/dansbecker/learning-to-use-xgboost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
