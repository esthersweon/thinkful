{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras for Neural Networks - Guided Example\n",
    "\n",
    "## MLP (Multi-Layer Perceptron)\n",
    "\n",
    "MLP Neural Networks are a set of perceptron models organized into layers, with one layer feeding into the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 60,000 images with 28*28 pixels (784 pixels total)\n",
    "# Need 60,000 arrays (1 per image) of length 784\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize values (0 ~ 255)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Print sample sizes\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices (i.e. dummify class column)\n",
    "# 1 column with 10 values -> 10 binary columns\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.4236 - acc: 0.8765 - val_loss: 0.1925 - val_acc: 0.9424\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2040 - acc: 0.9402 - val_loss: 0.1449 - val_acc: 0.9557\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1568 - acc: 0.9521 - val_loss: 0.1185 - val_acc: 0.9635\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1300 - acc: 0.9603 - val_loss: 0.1081 - val_acc: 0.9663\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1127 - acc: 0.9657 - val_loss: 0.1015 - val_acc: 0.9701\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1023 - acc: 0.9693 - val_loss: 0.0958 - val_acc: 0.9718\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0952 - acc: 0.9707 - val_loss: 0.0883 - val_acc: 0.9730\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0873 - acc: 0.9734 - val_loss: 0.0878 - val_acc: 0.9745\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0821 - acc: 0.9750 - val_loss: 0.0891 - val_acc: 0.9738\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0772 - acc: 0.9757 - val_loss: 0.0862 - val_acc: 0.9777\n",
      "Test loss: 0.08624280345954467\n",
      "Test accuracy: 0.9777\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "Convolution takes your data and creates overlapping subsegments testing for a given feature in a set of spaces and upon which it develops its model.\n",
    "\n",
    "First, you have to define a shape of your input data. This can theoretically be in any number of dimensions, though for our image example we will use 2d, since images are in two dimensions. \n",
    "\n",
    "Over that shaped data, we then create our tiles, also called __kernels__. These kernels are like little windows, that will look over subsets of the data of a given size. In the example below we create 3x3 kernels, which run overlapping over the whole 28x28 input looking for features. That is the convolutional layer, a way of searching for a subpattern over the whole of the image. \n",
    "\n",
    "Next comes a pooling layer. This is a downsampling technique, which effectively serves to reduce sample size and simplify later processes. For each value generated by our convolutional layers, it looks over the grid in non-overlapping segments and takes the maximum value of those outputs. It's not the features' exact location that matters, but its approximate or relative location. After pooling, you will flatten the data back out, so  it can be put into dense layers as in MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_rows, img_cols = 28, 28\n",
    "# num_classes = 10\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# if K.image_data_format() == 'channels_first':\n",
    "#     x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "#     input_shape = (1, img_rows, img_cols)\n",
    "# else:\n",
    "#     x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "#     input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print(x_train.shape[0], 'train samples')\n",
    "# print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# # convert class vectors to binary class matrices\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# # Building the Model\n",
    "# model = Sequential()\n",
    "# # First convolutional layer, note the specification of shape\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=128,\n",
    "#           epochs=10,\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test))\n",
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Recurrrent Neural Networks\n",
    "\n",
    "So far when we've talked about neural networks we've talked about them as feedforward. Recurrent neural networks do not obey that directional logic, instead letting the data cycle through the network.\n",
    "\n",
    "You have to use recurrent layers and often time distribution (which handles the extra dimension created through the LTSM layer, as a time dimension) to get these things running. You can find an example of a hierarchical recurrent network below (via the link here). When you get comfortable with networks as they exist in Keras for both convolution and MLP, start exploring recurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# num_classes = 10\n",
    "# epochs = 3\n",
    "\n",
    "# # Embedding dimensions.\n",
    "# row_hidden = 32\n",
    "# col_hidden = 32\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# # Reshapes data to 4D for Hierarchical RNN.\n",
    "# x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "# x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print(x_train.shape[0], 'train samples')\n",
    "# print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# # Converts class vectors to binary class matrices.\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "# # 4D input.\n",
    "# x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# # Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "# encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# # Encodes columns of encoded rows.\n",
    "# encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# # Final predictions and model\n",
    "# prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "# model = Model(x, prediction)\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test))\n",
    "\n",
    "# scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print('Test loss:', scores[0])\n",
    "# print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill: 99% MLP\n",
    "\n",
    "We have the MLP above, which runs reasonably quickly. Copy that code down here and see if you can tune it to achieve 99% accuracy with a Multi-Layer Perceptron. Does it run faster than the recurrent or convolutional neural nets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_81 (Dense)             (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0818 - acc: 0.9830 - val_loss: 0.1421 - val_acc: 0.9758\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0813 - acc: 0.9827 - val_loss: 0.1615 - val_acc: 0.9761\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0873 - acc: 0.9819 - val_loss: 0.1690 - val_acc: 0.9704\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0875 - acc: 0.9815 - val_loss: 0.1360 - val_acc: 0.9759\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0899 - acc: 0.9817 - val_loss: 0.1744 - val_acc: 0.9735\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1002 - acc: 0.9813 - val_loss: 0.1532 - val_acc: 0.9767\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1002 - acc: 0.9812 - val_loss: 0.1545 - val_acc: 0.9750\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1088 - acc: 0.9799 - val_loss: 0.1663 - val_acc: 0.9744\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1042 - acc: 0.9805 - val_loss: 0.1584 - val_acc: 0.9763\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.1084 - acc: 0.9811 - val_loss: 0.1671 - val_acc: 0.9733\n",
      "Test loss: 0.1671380371674531\n",
      "Test accuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=30, epochs=10, verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
