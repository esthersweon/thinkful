{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As supervised problem \n",
    "\n",
    "## Challenge 0\n",
    "\n",
    "Recall that the logistic regression model's best performance on the test set was 93%. See what you can do to improve performance. \n",
    "\n",
    "Suggested avenues of investigation: \n",
    "- Other modeling techniques (SVM?)\n",
    "- More features that take advantage of the spaCy information (include grammar, phrases, POS, etc)\n",
    "- Making sentence-level features (number of words, amount of punctuation)\n",
    "- Including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc)\n",
    "\n",
    "Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data (remove chapter lines)\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "alice = text_cleaner(alice)\n",
    "\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "emma = text_cleaner(emma)\n",
    "\n",
    "shakespeare = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "shakespeare = re.sub(r'Actus \\w+\\.', '', shakespeare)\n",
    "shakespeare = re.sub(r'Scoena \\w+\\.', '', shakespeare)\n",
    "shakespeare = text_cleaner(shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "persuasion_doc = nlp(persuasion)\n",
    "alice_doc = nlp(alice)\n",
    "emma_doc = nlp(emma)\n",
    "shakespeare_doc = nlp(shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                         (I, shall, be, late, !, ')  Carroll"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "shakespeare_sents = [[sent, \"Shakespeare\"] for sent in shakespeare_doc.sents]\n",
    "\n",
    "# FOR LATER USE\n",
    "alice_vs_other_sentences = pd.DataFrame(alice_sents + shakespeare_sents)\n",
    "persuasion_vs_other_sentences = pd.DataFrame(persuasion_sents + shakespeare_sents)\n",
    "austen_vs_other_sentences = pd.DataFrame(persuasion_sents + emma_sents + shakespeare_sents)\n",
    "\n",
    "# Combine the sentences from 2 novels into 1 data frame\n",
    "sentences = pd.DataFrame(persuasion_sents + alice_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (not token.is_punct and not token.is_stop and token.lemma_ in common_words)]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 100 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "\n",
    "# FOR LATER USE\n",
    "emmawords = bag_of_words(emma_doc)\n",
    "shakespearewords = bag_of_words(shakespeare_doc)\n",
    "common_alice_vs_other_words = set(alicewords + shakespearewords)\n",
    "common_persuasion_vs_other_words = set(persuasionwords + shakespearewords)\n",
    "common_austen_vs_other_words = set(persuasionwords + emmawords + shakespearewords)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n",
      "Processing row 650\n",
      "Processing row 700\n",
      "Processing row 750\n",
      "Processing row 800\n",
      "Processing row 850\n",
      "Processing row 900\n",
      "Processing row 950\n",
      "Processing row 1000\n",
      "Processing row 1050\n",
      "Processing row 1100\n",
      "Processing row 1150\n",
      "Processing row 1200\n",
      "Processing row 1250\n",
      "Processing row 1300\n",
      "Processing row 1350\n",
      "Processing row 1400\n",
      "Processing row 1450\n",
      "Processing row 1500\n",
      "Processing row 1550\n",
      "Processing row 1600\n",
      "Processing row 1650\n",
      "Processing row 1700\n",
      "Processing row 1750\n",
      "Processing row 1800\n",
      "Processing row 1850\n"
     ]
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will test how an SVM model would do on this same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel = 'linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "print('Training:', svm.score(X_train, y_train))\n",
    "print('Test:', svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will add sentence-level features like # of words, characters, and punctuation, to see if this will improve my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts['# words'] = word_counts['text_sentence'].str.len()\n",
    "word_counts['# chars'] = word_counts['text_sentence'].apply(lambda x: sum(list(map(lambda y:y.__len__(), x))))\n",
    "\n",
    "# Utility function to create a list of used punctuation\n",
    "def bag_of_punctuations(text):\n",
    "    all_punctuation = [token for token in text if token.is_punct]\n",
    "    return [item[0] for item in Counter(all_punctuation)]\n",
    "    \n",
    "def add_punctuation_features(df):\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        punctuations = [for token in sentence if token.is_punct]\n",
    "\n",
    "        for punctuation in punctuations:\n",
    "            df.loc[i, punctuation] += 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "word_counts = add_punctuation_features(word_counts)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "\n",
    "svm_with_sent_features = SVC(kernel = 'linear')\n",
    "svm_with_sent_features.fit(X_train, y_train)\n",
    "\n",
    "print('Training:', svm_with_sent_features.score(X_train, y_train))\n",
    "print('Test:', svm_with_sent_features.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_vs_other_word_counts = bow_features(alice_vs_other_sentences, common_alice_vs_other_words)\n",
    "alice_vs_other_word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persuasion_vs_other_word_counts = bow_features(persuasion_vs_other_sentences, common_persuasion_vs_other_words)\n",
    "persuasion_vs_other_word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen_vs_other_word_counts = bow_features(austen_vs_other_sentences, common_austen_vs_other_words)\n",
    "austen_vs_other_word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_SVM(df):\n",
    "    Y = df['text_source']\n",
    "    X = np.array(df.drop(['text_sentence','text_source'], 1))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "    \n",
    "    svm = SVC(kernel = 'linear')\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    print('Training:', svm.score(X_train, y_train))\n",
    "    print('Test:', svm.score(X_test, y_test))\n",
    "\n",
    "print('Alice in Wonderland vs. Other Work\\n')\n",
    "print(train_with_SVM(alice_vs_other_word_counts))\n",
    "\n",
    "print('Persuasion vs. Other Work\\n')\n",
    "print(train_with_SVM(persuasion_vs_other_word_counts))\n",
    "\n",
    "print('Austen vs. Other Work\\n')\n",
    "print(train_with_SVM(austen_vs_other_word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
